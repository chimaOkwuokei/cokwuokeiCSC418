{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8f6661-7933-406a-a7d8-be0bf3c8be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox, filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6459125c-6f5d-4d75-a72c-0c050dd4a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet('cfg/yolov3.weights', 'cfg/yolov3.cfg')\n",
    "with open('cfg/coco.names', 'r') as f:\n",
    "    classes = f.read().splitlines()\n",
    "# colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# frames are like snapshots of images in the videos\n",
    "def detect_objects_yolo(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416,416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "    layer_outputs = net.forward(output_layers_names)\n",
    "\n",
    "    boxes = [] \n",
    "    confidences = [] \n",
    "    class_ids = []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    #non max suppression to remove the redundant overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        confidence = str(round(confidences[i], 2))\n",
    "        color = colors[i]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(frame,  label + \" \" + confidence, (x, y + 20), font, 2, (255,255,255), 2)\n",
    "\n",
    "    return frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf037ffd-2c08-4a5e-ad20-b7471c6172ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"PAU Object Detection - YOLO\")\n",
    "        self.root.geometry(\"700x650\")\n",
    "\n",
    "        # Add your video file paths here\n",
    "        self.video_paths = {\n",
    "            \"Test 1\": \"videos/1.mp4\",\n",
    "            \"Test 2\": \"videos/2.mp4\",\n",
    "            \"Test 3\": \"videos/3.mp4\"\n",
    "        }\n",
    "\n",
    "        self.selected_video = None\n",
    "        self.cap = None\n",
    "        self.running = False\n",
    "\n",
    "        self.create_widgets()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        tk.Label(self.root, text=\"Select one of your favorite PAU locations:\").pack(pady=10)\n",
    "\n",
    "        btn_frame = tk.Frame(self.root)\n",
    "        btn_frame.pack()\n",
    "\n",
    "        for name, path in self.video_paths.items():\n",
    "            tk.Button(btn_frame, text=name, command=lambda p=path: self.select_video(p)).pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        self.canvas = tk.Canvas(self.root, width=640, height=480, bg=\"grey\")\n",
    "        self.canvas.pack(pady=20)\n",
    "\n",
    "        control_frame = tk.Frame(self.root)\n",
    "        control_frame.pack()\n",
    "\n",
    "        tk.Button(control_frame, text=\"Detect Objects\", command=self.start_detection).pack(side=tk.LEFT, padx=10)\n",
    "        tk.Button(control_frame, text=\"Pause\", command=self.pause_video).pack(side=tk.LEFT, padx=10)\n",
    "        tk.Button(control_frame, text=\"Stop\", command=self.stop_video).pack(side=tk.LEFT, padx=10)\n",
    "        tk.Button(control_frame, text=\"Close\", command=self.close_app).pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "    def select_video(self, path):\n",
    "        self.selected_video = path\n",
    "        messagebox.showinfo(\"Video Selected\", f\"Selected video: {os.path.basename(path)}\")\n",
    "\n",
    "    def start_detection(self):\n",
    "        if not self.selected_video:\n",
    "            messagebox.showerror(\"Error\", \"Please select a video first.\")\n",
    "            return\n",
    "        self.running = True\n",
    "        self.cap = cv2.VideoCapture(self.selected_video)\n",
    "        threading.Thread(target=self.process_video, daemon=True).start()\n",
    "\n",
    "    def pause_video(self):\n",
    "        if self.running:\n",
    "            self.running = False\n",
    "        else:\n",
    "            if self.cap:\n",
    "                self.running = True\n",
    "                threading.Thread(target=self.process_video, daemon=True).start()\n",
    "\n",
    "    def stop_video(self):\n",
    "        self.running = False\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.canvas.delete(\"all\")\n",
    "\n",
    "    def close_app(self):\n",
    "        self.stop_video()\n",
    "        self.root.destroy()\n",
    "\n",
    "    #puts the video into individual frames which serves as images\n",
    "    def process_video(self):\n",
    "        while self.cap.isOpened() and self.running:\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.resize(frame, (640, 480))\n",
    "            frame = detect_objects_yolo(frame)\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img_pil = Image.fromarray(frame_rgb)\n",
    "            imgtk = ImageTk.PhotoImage(image=img_pil)\n",
    "\n",
    "            self.canvas.create_image(0, 0, anchor=tk.NW, image=imgtk)\n",
    "            self.canvas.image = imgtk\n",
    "\n",
    "            if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "# Run the app\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ObjectDetectionApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cdfbed-6046-49b0-9170-11d7b819ce5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
